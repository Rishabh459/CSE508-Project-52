{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ussin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "import re\n",
    "# import nltk libraries for preprocessing\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('News Articles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a preprocess function that takes in a string and returns a list of words\n",
    "def preprocess(text):\n",
    "    # remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    # make all words lowercase\n",
    "    text = text.lower()\n",
    "    # split into a list of words\n",
    "    text = text.split()\n",
    "    # remove stopwords\n",
    "    text = [word for word in text if word not in stopwords.words('english')]\n",
    "    # remove words with length==1\n",
    "    text = [word for word in text if len(word)>1]\n",
    "    # lemmatize words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = [lemmatizer.lemmatize(word) for word in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open files in the directory each file contains add en extra section in dictionary of the articles with the key as 'article-id' and vallue as the file name+article number\n",
    "# for example if the file name is 'abc-news.json' and the article number is 1 then the key value pair will be 'article-id':'abc-news/1'\n",
    "# append this key in the dictionary of each article in the file and update the file with the new dictionary as json\n",
    "def update():\n",
    "    for file in os.listdir():\n",
    "        if len(file.split('_')) > 1:\n",
    "                continue\n",
    "        with open(file, 'r') as f:\n",
    "            data_dict = json.load(f)\n",
    "        for i in range(len(data_dict['articles'])):\n",
    "            data_dict['articles'][i]['article-id'] = file.split('.')[0]+'/'+str(i+1)\n",
    "            f.close()\n",
    "        with open(file, 'w') as f:\n",
    "            json.dump(data_dict, f, indent = 4)\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI.json\n",
      "AI_preprocessed.json\n",
      "business.json\n",
      "business_preprocessed.json\n",
      "chess.json\n",
      "chess_preprocessed.json\n",
      "climate change.json\n",
      "climate change_preprocessed.json\n",
      "comedy.json\n",
      "comedy_preprocessed.json\n",
      "cricket.json\n",
      "cricket_preprocessed.json\n",
      "crypto.json\n",
      "crypto_preprocessed.json\n",
      "entertainment.json\n",
      "entertainment_preprocessed.json\n",
      "environment.json\n",
      "environment_preprocessed.json\n",
      "politics.json\n",
      "politics_preprocessed.json\n",
      "preprocessed_files.json\n",
      "social justice.json\n",
      "social justice_preprocessed.json\n",
      "spacex.json\n",
      "spacex_preprocessed.json\n",
      "sports.json\n",
      "sports_preprocessed.json\n",
      "sustainable.json\n",
      "sustainable_preprocessed.json\n",
      "tech.json\n",
      "tech_preprocessed.json\n",
      "tesla.json\n",
      "tesla_preprocessed.json\n",
      "women rights.json\n",
      "women rights_preprocessed.json\n",
      "youth activism.json\n",
      "youth activism_preprocessed.json\n"
     ]
    }
   ],
   "source": [
    "# now we preprocess the text in each article and store it in a dictionary with key as the article-id and value as the list of words\n",
    "# send the author title description and content of each article to the preprocess function and store the list of words in a dictionary\n",
    "# with key as the article-id and value as the list of words in a new file called 'filename_preprocessed.json'\n",
    "def preprocess_articles():\n",
    "    preprocessed_dict = {}\n",
    "    for file in os.listdir():\n",
    "        print(file)\n",
    "        with open(file, 'r') as f:\n",
    "            if len(file.split('_')) > 1:\n",
    "                continue\n",
    "            data_dict = json.load(f)\n",
    "            for i in range(len(data_dict['articles'])):\n",
    "                # only preprocess article section that are not empty\n",
    "                preprocessed_dict[data_dict['articles'][i]['article-id']] = []\n",
    "                if data_dict['articles'][i]['content'] != None:\n",
    "                    preprocessed_dict[data_dict['articles'][i]['article-id']].extend(preprocess(data_dict['articles'][i]['content']))\n",
    "                if data_dict['articles'][i]['title'] != None:\n",
    "                    preprocessed_dict[data_dict['articles'][i]['article-id']].extend(preprocess(data_dict['articles'][i]['title']))\n",
    "                if data_dict['articles'][i]['description'] != None:\n",
    "                    preprocessed_dict[data_dict['articles'][i]['article-id']].extend(preprocess(data_dict['articles'][i]['description']))\n",
    "                if data_dict['articles'][i]['author'] != None:\n",
    "                    preprocessed_dict[data_dict['articles'][i]['article-id']].extend(preprocess(data_dict['articles'][i]['author']))\n",
    "            f.close()\n",
    "    with open('preprocessed_files.json', 'w') as f:\n",
    "        json.dump(preprocessed_dict, f, indent = 4)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we create a dictionary with key as the document id and value as along with the link of the article only and no other information and save the whole dict as map.json\n",
    "def create_map():\n",
    "    map_dict = {}\n",
    "    for file in os.listdir():\n",
    "        if len(file.split('_')) > 1:\n",
    "            continue\n",
    "        with open(file, 'r') as f:\n",
    "            data_dict = json.load(f)\n",
    "            for i in range(len(data_dict['articles'])):\n",
    "                map_dict[data_dict['articles'][i]['article-id']] = data_dict['articles'][i]['url']\n",
    "    with open('maplink.json', 'w') as f:\n",
    "        json.dump(map_dict, f, indent = 4)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary for term frequency that has key as the article-id and value as a dictionary with key as the word and value as the frequency of the word in the article\n",
    "\n",
    "def create_tf():\n",
    "    tf_dict = {}\n",
    "    with open('preprocessed_files.json', 'r') as f:\n",
    "        preprocessed_dict = json.load(f)\n",
    "        for key in preprocessed_dict.keys():\n",
    "            tf_dict[key] = {}\n",
    "            for word in preprocessed_dict[key]:\n",
    "                if word in tf_dict[key].keys():\n",
    "                    tf_dict[key][word] += 1\n",
    "                else:\n",
    "                    tf_dict[key][word] = 1\n",
    "    with open('tf.json', 'w') as f:\n",
    "        json.dump(tf_dict, f, indent = 4)\n",
    "        f.close()\n",
    "\n",
    "# create_tf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create posting list that has key as the word and value as a list of article-ids in which the word is present and the number of documents in which the word is present\n",
    "def create_posting_list():\n",
    "    posting_list = {}\n",
    "    with open('preprocessed_files.json', 'r') as f:\n",
    "        preprocessed_dict = json.load(f)\n",
    "        for key in preprocessed_dict.keys():\n",
    "            for word in preprocessed_dict[key]:\n",
    "                if word in posting_list.keys():\n",
    "                    posting_list[word][0].append(key)\n",
    "                    posting_list[word][1] += 1\n",
    "                else:\n",
    "                    posting_list[word] = [[key], 1]\n",
    "    with open('posting_list.json', 'w') as f:\n",
    "        json.dump(posting_list, f, indent = 4)\n",
    "        f.close()\n",
    "\n",
    "# create idf dict\n",
    "def create_idf_dict(no_of_docs, posting_list):\n",
    "    idf_dict = {}\n",
    "    for token in posting_list:\n",
    "        idf_dict[token] = math.log(no_of_docs / posting_list[token][1]+1)\n",
    "    with open('idf.json', 'w') as f:\n",
    "        json.dump(idf_dict, f, indent = 4)\n",
    "        f.close()\n",
    "\n",
    "posting_list = json.load(open('posting_list.json', 'r'))\n",
    "number_of_docs = len(json.load(open('preprocessed_files.json', 'r')))\n",
    "create_idf_dict(number_of_docs, posting_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tf-idf matrix for each document and \n",
    "# Weighting Scheme TF Weight\n",
    "# Binary 0,1\n",
    "# Raw count f(t,d)\n",
    "# Term frequency f(t,d)/Pf(t‘, d)\n",
    "# Log normalization log(1+f(t,d))\n",
    "# Double normalization 0.5+0.5*(f(t,d)/ max(f(t‘,d))\n",
    "\n",
    "def create_tf_idf_matrix_binary(no_of_docs, tf_dict, idf_dict):\n",
    "    vocab_size = len(idf_dict)\n",
    "    tf_idf_matrix = np.zeros((no_of_docs, vocab_size))\n",
    "    # check for all terms in the idf_dict, if it is present in the tf_dict of the document, then set the value to 1*idf\n",
    "    for i, filename in enumerate(tqdm(tf_dict.keys())):\n",
    "        for j, token in enumerate(idf_dict):\n",
    "            if token in tf_dict[filename]:\n",
    "                tf_idf_matrix[i][j] = 1*idf_dict[token]\n",
    "                \n",
    "    return tf_idf_matrix\n",
    "\n",
    "def create_tf_idf_matrix_raw_count(no_of_docs, tf_dict, idf_dict):\n",
    "    vocab_size = len(idf_dict)\n",
    "    tf_idf_matrix = np.zeros((no_of_docs, vocab_size))\n",
    "    for i, filename in enumerate(tqdm(tf_dict.keys())):\n",
    "        for j, token in enumerate(idf_dict):\n",
    "            if token in tf_dict[filename]:\n",
    "                tf_idf_matrix[i][j] = tf_dict[filename][token]*idf_dict[token]\n",
    "                \n",
    "    return tf_idf_matrix\n",
    "\n",
    "def create_tf_idf_matrix_term_frequency(no_of_docs, tf_dict, idf_dict):\n",
    "    vocab_size = len(idf_dict)\n",
    "    tf_idf_matrix = np.zeros((no_of_docs, vocab_size))\n",
    "    for i, filename in enumerate(tqdm(tf_dict.keys())):\n",
    "        for j, token in enumerate(idf_dict):\n",
    "            if token in tf_dict[filename]:\n",
    "                tf_idf_matrix[i][j] = (tf_dict[filename][token]/len(tf_dict[filename]))*idf_dict[token]\n",
    "\n",
    "    return tf_idf_matrix\n",
    "\n",
    "def create_tf_idf_matrix_log_normalization(no_of_docs, tf_dict, idf_dict):\n",
    "    vocab_size = len(idf_dict)\n",
    "    tf_idf_matrix = np.zeros((no_of_docs, vocab_size))\n",
    "    for i, filename in enumerate(tqdm(tf_dict.keys())):\n",
    "        for j, token in enumerate(idf_dict):\n",
    "            if token in tf_dict[filename]:\n",
    "                tf_idf_matrix[i][j] = math.log(1+tf_dict[filename][token])*idf_dict[token]\n",
    "    \n",
    "    return tf_idf_matrix\n",
    "\n",
    "def create_tf_idf_matrix_double_normalization(no_of_docs, tf_dict, idf_dict):\n",
    "    vocab_size = len(idf_dict)\n",
    "    tf_idf_matrix = np.zeros((no_of_docs, vocab_size))\n",
    "    for i, filename in enumerate(tqdm(tf_dict.keys())):\n",
    "        max_tf = 0\n",
    "        for token in tf_dict[filename]:\n",
    "            if tf_dict[filename][token] > max_tf:\n",
    "                max_tf = tf_dict[filename][token]\n",
    "        for j, token in enumerate(idf_dict):\n",
    "            if token in tf_dict[filename]:\n",
    "                tf_idf_matrix[i][j] = 0.5+0.5*(tf_dict[filename][token]/max_tf)*idf_dict[token]\n",
    "    return tf_idf_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1800/1800 [00:07<00:00, 250.17it/s]\n",
      "100%|██████████| 1800/1800 [00:04<00:00, 408.56it/s]\n",
      "100%|██████████| 1800/1800 [00:04<00:00, 434.60it/s]\n",
      "100%|██████████| 1800/1800 [00:04<00:00, 428.04it/s]\n",
      "100%|██████████| 1800/1800 [00:04<00:00, 418.48it/s]\n"
     ]
    }
   ],
   "source": [
    "# create tf-idf matrix\n",
    "binary_tf_idf_matrix = create_tf_idf_matrix_binary(number_of_docs, json.load(open('tf.json', 'r')), json.load(open('idf.json', 'r')))\n",
    "raw_count_tf_idf_matrix = create_tf_idf_matrix_raw_count(number_of_docs, json.load(open('tf.json', 'r')), json.load(open('idf.json', 'r')))\n",
    "term_frequency_tf_idf_matrix = create_tf_idf_matrix_term_frequency(number_of_docs, json.load(open('tf.json', 'r')), json.load(open('idf.json', 'r')))\n",
    "log_normalization_tf_idf_matrix = create_tf_idf_matrix_log_normalization(number_of_docs, json.load(open('tf.json', 'r')), json.load(open('idf.json', 'r')))\n",
    "double_normalization_tf_idf_matrix = create_tf_idf_matrix_double_normalization(number_of_docs, json.load(open('tf.json', 'r')), json.load(open('idf.json', 'r')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
