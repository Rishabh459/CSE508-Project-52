{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import re\n",
    "# import nltk libraries for preprocessing\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import json\n",
    "\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('News Articles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a preprocess function that takes in a string and returns a list of words\n",
    "def preprocess(text):\n",
    "    # remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    # make all words lowercase\n",
    "    text = text.lower()\n",
    "    # split into a list of words\n",
    "    text = text.split()\n",
    "    # remove stopwords\n",
    "    text = [word for word in text if word not in stopwords.words('english')]\n",
    "    # remove words with length==1\n",
    "    text = [word for word in text if len(word)>1]\n",
    "    # lemmatize words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = [lemmatizer.lemmatize(word) for word in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open files in the directory each file contains add en extra section in dictionary of the articles with the key as 'article-id' and vallue as the file name+article number\n",
    "# for example if the file name is 'abc-news.json' and the article number is 1 then the key value pair will be 'article-id':'abc-news/1'\n",
    "# append this key in the dictionary of each article in the file and update the file with the new dictionary as json\n",
    "def update():\n",
    "    for file in os.listdir():\n",
    "        if len(file.split('_')) > 1:\n",
    "                continue\n",
    "        with open(file, 'r') as f:\n",
    "            data_dict = json.load(f)\n",
    "        for i in range(len(data_dict['articles'])):\n",
    "            data_dict['articles'][i]['article-id'] = file.split('.')[0]+'/'+str(i+1)\n",
    "            f.close()\n",
    "        with open(file, 'w') as f:\n",
    "            json.dump(data_dict, f, indent = 4)\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI.json\n",
      "AI_preprocessed.json\n",
      "business.json\n",
      "business_preprocessed.json\n",
      "chess.json\n",
      "chess_preprocessed.json\n",
      "climate change.json\n",
      "climate change_preprocessed.json\n",
      "comedy.json\n",
      "comedy_preprocessed.json\n",
      "cricket.json\n",
      "cricket_preprocessed.json\n",
      "crypto.json\n",
      "crypto_preprocessed.json\n",
      "entertainment.json\n",
      "entertainment_preprocessed.json\n",
      "environment.json\n",
      "environment_preprocessed.json\n",
      "politics.json\n",
      "politics_preprocessed.json\n",
      "preprocessed_files.json\n",
      "social justice.json\n",
      "social justice_preprocessed.json\n",
      "spacex.json\n",
      "spacex_preprocessed.json\n",
      "sports.json\n",
      "sports_preprocessed.json\n",
      "sustainable.json\n",
      "sustainable_preprocessed.json\n",
      "tech.json\n",
      "tech_preprocessed.json\n",
      "tesla.json\n",
      "tesla_preprocessed.json\n",
      "women rights.json\n",
      "women rights_preprocessed.json\n",
      "youth activism.json\n",
      "youth activism_preprocessed.json\n"
     ]
    }
   ],
   "source": [
    "# now we preprocess the text in each article and store it in a dictionary with key as the article-id and value as the list of words\n",
    "# send the author title description and content of each article to the preprocess function and store the list of words in a dictionary\n",
    "# with key as the article-id and value as the list of words in a new file called 'filename_preprocessed.json'\n",
    "def preprocess_articles():\n",
    "    preprocessed_dict = {}\n",
    "    for file in os.listdir():\n",
    "        print(file)\n",
    "        with open(file, 'r') as f:\n",
    "            if len(file.split('_')) > 1:\n",
    "                continue\n",
    "            data_dict = json.load(f)\n",
    "            for i in range(len(data_dict['articles'])):\n",
    "                # only preprocess article section that are not empty\n",
    "                preprocessed_dict[data_dict['articles'][i]['article-id']] = []\n",
    "                if data_dict['articles'][i]['content'] != None:\n",
    "                    preprocessed_dict[data_dict['articles'][i]['article-id']].extend(preprocess(data_dict['articles'][i]['content']))\n",
    "                if data_dict['articles'][i]['title'] != None:\n",
    "                    preprocessed_dict[data_dict['articles'][i]['article-id']].extend(preprocess(data_dict['articles'][i]['title']))\n",
    "                if data_dict['articles'][i]['description'] != None:\n",
    "                    preprocessed_dict[data_dict['articles'][i]['article-id']].extend(preprocess(data_dict['articles'][i]['description']))\n",
    "                if data_dict['articles'][i]['author'] != None:\n",
    "                    preprocessed_dict[data_dict['articles'][i]['article-id']].extend(preprocess(data_dict['articles'][i]['author']))\n",
    "            f.close()\n",
    "    with open('preprocessed_files.json', 'w') as f:\n",
    "        json.dump(preprocessed_dict, f, indent = 4)\n",
    "        f.close()\n",
    "\n",
    "preprocess_articles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we create a dictionary with key as the document id and value as along with the link of the article only and no other information and save the whole dict as map.json\n",
    "def create_map():\n",
    "    map_dict = {}\n",
    "    for file in os.listdir():\n",
    "        if len(file.split('_')) > 1:\n",
    "            continue\n",
    "        with open(file, 'r') as f:\n",
    "            data_dict = json.load(f)\n",
    "            for i in range(len(data_dict['articles'])):\n",
    "                map_dict[data_dict['articles'][i]['article-id']] = data_dict['articles'][i]['url']\n",
    "    with open('maplink.json', 'w') as f:\n",
    "        json.dump(map_dict, f, indent = 4)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tf dictionary with key as the article-id and value as a dictionary with key as the word and value as the tf of the word in the article\n",
    "def create_tf():\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
