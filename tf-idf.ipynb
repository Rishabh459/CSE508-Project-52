{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ussin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "import re\n",
    "# import nltk libraries for preprocessing\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "os.chdir('News Articles')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a preprocess function that takes in a string and returns a list of words\n",
    "def preprocess(text):\n",
    "    # remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    # make all words lowercase\n",
    "    text = text.lower()\n",
    "    # split into a list of words\n",
    "    text = text.split()\n",
    "    # remove stopwords\n",
    "    text = [word for word in text if word not in stopwords.words('english')]\n",
    "    # remove words with length==1\n",
    "    text = [word for word in text if len(word)>1]\n",
    "    # lemmatize words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = [lemmatizer.lemmatize(word) for word in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open files in the directory each file contains add en extra section in dictionary of the articles with the key as 'article-id' and vallue as the file name+article number\n",
    "# for example if the file name is 'abc-news.json' and the article number is 1 then the key value pair will be 'article-id':'abc-news/1'\n",
    "# append this key in the dictionary of each article in the file and update the file with the new dictionary as json\n",
    "def update():\n",
    "    for file in os.listdir():\n",
    "        if len(file.split('_')) > 1:\n",
    "                continue\n",
    "        with open(file, 'r') as f:\n",
    "            data_dict = json.load(f)\n",
    "        for i in range(len(data_dict['articles'])):\n",
    "            data_dict['articles'][i]['article-id'] = file.split('.')[0]+'/'+str(i+1)\n",
    "            f.close()\n",
    "        with open(file, 'w') as f:\n",
    "            json.dump(data_dict, f, indent = 4)\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we preprocess the text in each article and store it in a dictionary with key as the article-id and value as the list of words\n",
    "# send the author title description and content of each article to the preprocess function and store the list of words in a dictionary\n",
    "# with key as the article-id and value as the list of words in a new file called 'filename_preprocessed.json'\n",
    "def preprocess_articles():\n",
    "    preprocessed_dict = {}\n",
    "    for file in os.listdir():\n",
    "        print(file)\n",
    "        with open(file, 'r') as f:\n",
    "            if len(file.split('_')) > 1:\n",
    "                continue\n",
    "            data_dict = json.load(f)\n",
    "            for i in range(len(data_dict['articles'])):\n",
    "                # only preprocess article section that are not empty\n",
    "                preprocessed_dict[data_dict['articles'][i]['article-id']] = []\n",
    "                if data_dict['articles'][i]['content'] != None:\n",
    "                    preprocessed_dict[data_dict['articles'][i]['article-id']].extend(preprocess(data_dict['articles'][i]['content']))\n",
    "                if data_dict['articles'][i]['title'] != None:\n",
    "                    preprocessed_dict[data_dict['articles'][i]['article-id']].extend(preprocess(data_dict['articles'][i]['title']))\n",
    "                if data_dict['articles'][i]['description'] != None:\n",
    "                    preprocessed_dict[data_dict['articles'][i]['article-id']].extend(preprocess(data_dict['articles'][i]['description']))\n",
    "                if data_dict['articles'][i]['author'] != None:\n",
    "                    preprocessed_dict[data_dict['articles'][i]['article-id']].extend(preprocess(data_dict['articles'][i]['author']))\n",
    "            f.close()\n",
    "    with open('preprocessed_files.json', 'w') as f:\n",
    "        json.dump(preprocessed_dict, f, indent = 4)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we create a dictionary with key as the document id and value as along with \n",
    "# the link of the article only and no other information and save the whole dict as map.json\n",
    "def create_map():\n",
    "    map_dict = {}\n",
    "    queries = ['crypto', 'tech', 'business', 'spacex', 'tesla', 'politics', 'AI', 'environment', 'youth activism', 'sustainable', 'climate change', 'social justice', 'women rights', 'sports', 'entertainment', 'cricket', 'comedy', 'chess']\n",
    "    for file in os.listdir():\n",
    "        if file.split('.')[0] not in queries:\n",
    "            continue\n",
    "        with open(file, 'r') as f:\n",
    "            data_dict = json.load(f)\n",
    "            for i in range(len(data_dict['articles'])):\n",
    "                map_dict[data_dict['articles'][i]['article-id']] = data_dict['articles'][i]['url']\n",
    "    with open('maplink.json', 'w') as f:\n",
    "        json.dump(map_dict, f, indent = 4)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary for term frequency that has key as the article-id and value as a dictionary with key as the word and value as the frequency of the word in the article\n",
    "def create_tf():\n",
    "    tf_dict = {}\n",
    "    with open('preprocessed_files.json', 'r') as f:\n",
    "        preprocessed_dict = json.load(f)\n",
    "        for key in preprocessed_dict.keys():\n",
    "            tf_dict[key] = {}\n",
    "            for word in preprocessed_dict[key]:\n",
    "                if word in tf_dict[key].keys():\n",
    "                    tf_dict[key][word] += 1\n",
    "                else:\n",
    "                    tf_dict[key][word] = 1\n",
    "    with open('tf.json', 'w') as f:\n",
    "        json.dump(tf_dict, f, indent = 4)\n",
    "        f.close()\n",
    "\n",
    "create_tf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create posting list that has key as the word and value as a list of article-ids in which the word is present and the number of documents in which the word is present\n",
    "def create_posting_list():\n",
    "    posting_list = {}\n",
    "    with open('preprocessed_files.json', 'r') as f:\n",
    "        preprocessed_dict = json.load(f)\n",
    "        for key in preprocessed_dict.keys():\n",
    "            for word in preprocessed_dict[key]:\n",
    "                if word in posting_list.keys():\n",
    "                    posting_list[word][0].append(key)\n",
    "                    posting_list[word][1] += 1\n",
    "                else:\n",
    "                    posting_list[word] = [[key], 1]\n",
    "    with open('posting_list.json', 'w') as f:\n",
    "        json.dump(posting_list, f, indent = 4)\n",
    "        f.close()\n",
    "\n",
    "# create idf dict\n",
    "def create_idf_dict(no_of_docs, posting_list):\n",
    "    idf_dict = {}\n",
    "    for token in posting_list:\n",
    "        idf_dict[token] = math.log(no_of_docs / posting_list[token][1]+1)\n",
    "    with open('idf.json', 'w') as f:\n",
    "        json.dump(idf_dict, f, indent = 4)\n",
    "        f.close()\n",
    "\n",
    "\n",
    "# create_posting_list()\n",
    "posting_list = json.load(open('posting_list.json', 'r'))\n",
    "number_of_docs = len(json.load(open('preprocessed_files.json', 'r')))\n",
    "# create_idf_dict(number_of_docs, posting_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tf-idf matrix for each document and \n",
    "# Weighting Scheme TF Weight\n",
    "# Binary 0,1\n",
    "# Raw count f(t,d)\n",
    "# Term frequency f(t,d)/Pf(t‘, d)\n",
    "# Log normalization log(1+f(t,d))\n",
    "# Double normalization 0.5+0.5*(f(t,d)/ max(f(t‘,d))\n",
    "\n",
    "def create_tf_idf_matrix_binary(no_of_docs, tf_dict, idf_dict):\n",
    "    vocab_size = len(idf_dict)\n",
    "    tf_idf_matrix = np.zeros((no_of_docs, vocab_size))\n",
    "    # check for all terms in the idf_dict, if it is present in the tf_dict of the document, then set the value to 1*idf\n",
    "    for i, filename in enumerate(tqdm(tf_dict.keys())):\n",
    "        for j, token in enumerate(idf_dict):\n",
    "            if token in tf_dict[filename]:\n",
    "                tf_idf_matrix[i][j] = 1*idf_dict[token]\n",
    "                \n",
    "    return tf_idf_matrix\n",
    "\n",
    "def create_tf_idf_matrix_raw_count(no_of_docs, tf_dict, idf_dict):\n",
    "    vocab_size = len(idf_dict)\n",
    "    tf_idf_matrix = np.zeros((no_of_docs, vocab_size))\n",
    "    for i, filename in enumerate(tqdm(tf_dict.keys())):\n",
    "        for j, token in enumerate(idf_dict):\n",
    "            if token in tf_dict[filename]:\n",
    "                tf_idf_matrix[i][j] = tf_dict[filename][token]*idf_dict[token]\n",
    "                \n",
    "    return tf_idf_matrix\n",
    "\n",
    "def create_tf_idf_matrix_term_frequency(no_of_docs, tf_dict, idf_dict):\n",
    "    vocab_size = len(idf_dict)\n",
    "    tf_idf_matrix = np.zeros((no_of_docs, vocab_size))\n",
    "    for i, filename in enumerate(tqdm(tf_dict.keys())):\n",
    "        for j, token in enumerate(idf_dict):\n",
    "            if token in tf_dict[filename]:\n",
    "                tf_idf_matrix[i][j] = (tf_dict[filename][token]/len(tf_dict[filename]))*idf_dict[token]\n",
    "\n",
    "    return tf_idf_matrix\n",
    "\n",
    "def create_tf_idf_matrix_log_normalization(no_of_docs, tf_dict, idf_dict):\n",
    "    vocab_size = len(idf_dict)\n",
    "    tf_idf_matrix = np.zeros((no_of_docs, vocab_size))\n",
    "    for i, filename in enumerate(tqdm(tf_dict.keys())):\n",
    "        for j, token in enumerate(idf_dict):\n",
    "            if token in tf_dict[filename]:\n",
    "                tf_idf_matrix[i][j] = math.log(1+tf_dict[filename][token])*idf_dict[token]\n",
    "    \n",
    "    return tf_idf_matrix\n",
    "\n",
    "def create_tf_idf_matrix_double_normalization(no_of_docs, tf_dict, idf_dict):\n",
    "    vocab_size = len(idf_dict)\n",
    "    tf_idf_matrix = np.zeros((no_of_docs, vocab_size))\n",
    "    for i, filename in enumerate(tqdm(tf_dict.keys())):\n",
    "        max_tf = 0\n",
    "        for token in tf_dict[filename]:\n",
    "            if tf_dict[filename][token] > max_tf:\n",
    "                max_tf = tf_dict[filename][token]\n",
    "        for j, token in enumerate(idf_dict):\n",
    "            if token in tf_dict[filename]:\n",
    "                tf_idf_matrix[i][j] = 0.5+0.5*(tf_dict[filename][token]/max_tf)*idf_dict[token]\n",
    "    return tf_idf_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1800/1800 [00:04<00:00, 430.11it/s]\n",
      "100%|██████████| 1800/1800 [00:04<00:00, 440.55it/s]\n",
      "100%|██████████| 1800/1800 [00:04<00:00, 385.26it/s]\n",
      "100%|██████████| 1800/1800 [00:03<00:00, 460.16it/s]\n",
      "100%|██████████| 1800/1800 [00:04<00:00, 449.15it/s]\n"
     ]
    }
   ],
   "source": [
    "# create tf-idf matrix\n",
    "binary_tf_idf_matrix = create_tf_idf_matrix_binary(number_of_docs, json.load(open('tf.json', 'r')), json.load(open('idf.json', 'r')))\n",
    "raw_count_tf_idf_matrix = create_tf_idf_matrix_raw_count(number_of_docs, json.load(open('tf.json', 'r')), json.load(open('idf.json', 'r')))\n",
    "term_frequency_tf_idf_matrix = create_tf_idf_matrix_term_frequency(number_of_docs, json.load(open('tf.json', 'r')), json.load(open('idf.json', 'r')))\n",
    "log_normalization_tf_idf_matrix = create_tf_idf_matrix_log_normalization(number_of_docs, json.load(open('tf.json', 'r')), json.load(open('idf.json', 'r')))\n",
    "double_normalization_tf_idf_matrix = create_tf_idf_matrix_double_normalization(number_of_docs, json.load(open('tf.json', 'r')), json.load(open('idf.json', 'r')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the similarity matrix in a pickle file\n",
    "with open('binary_tf_idf_matrix.pkl', 'wb') as f:\n",
    "    pickle.dump(binary_tf_idf_matrix, f)\n",
    "with open('raw_count_tf_idf_matrix.pkl', 'wb') as f:\n",
    "    pickle.dump(raw_count_tf_idf_matrix, f)\n",
    "with open('term_frequency_tf_idf_matrix.pkl', 'wb') as f:\n",
    "    pickle.dump(term_frequency_tf_idf_matrix, f)\n",
    "with open('log_normalization_tf_idf_matrix.pkl', 'wb') as f:\n",
    "    pickle.dump(log_normalization_tf_idf_matrix, f)\n",
    "with open('double_normalization_tf_idf_matrix.pkl', 'wb') as f:\n",
    "    pickle.dump(double_normalization_tf_idf_matrix, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of       Unnamed: 0                       Date      User  \\\n",
       "0              0  2023-03-23 21:55:32+00:00  elonmusk   \n",
       "1              1  2023-03-23 21:54:15+00:00  elonmusk   \n",
       "2              2  2023-03-23 21:31:38+00:00  elonmusk   \n",
       "3              3  2023-03-23 19:44:49+00:00  elonmusk   \n",
       "4              4  2023-03-23 09:31:25+00:00  elonmusk   \n",
       "...          ...                        ...       ...   \n",
       "1995        1995  2022-12-26 17:58:30+00:00  elonmusk   \n",
       "1996        1996  2022-12-26 17:56:10+00:00  elonmusk   \n",
       "1997        1997  2022-12-26 17:33:10+00:00  elonmusk   \n",
       "1998        1998  2022-12-26 17:29:43+00:00  elonmusk   \n",
       "1999        1999  2022-12-26 16:42:20+00:00  elonmusk   \n",
       "\n",
       "                                                  Tweet hashtag  \\\n",
       "0                                     @Chad_Hurley ð      []   \n",
       "1     @BillyM2k ð¶ Always look on the bright side ...      []   \n",
       "2             Twitter Verified now available worldwide!      []   \n",
       "3                   @lexfridman Not many years from now      []   \n",
       "4                   @RepJeffries Thatâs what she said      []   \n",
       "...                                                 ...     ...   \n",
       "1995  @WallStreetSilv Approaching 100 Starlinks acti...      []   \n",
       "1996           @esthercrawford I couldnât agree more!      []   \n",
       "1997                            @cb_doge @BillyM2k ð¤£      []   \n",
       "1998                                  @BillyM2k Exactly      []   \n",
       "1999  (Many of whom were, of course, actively suppre...      []   \n",
       "\n",
       "                                                   text  Segmented#  \n",
       "0                                                    []         NaN  \n",
       "1          ['always', 'look', 'bright', 'side', 'life']         NaN  \n",
       "2     ['twitter', 'verified', 'available', 'worldwide']         NaN  \n",
       "3                                      ['many', 'year']         NaN  \n",
       "4                                     ['thats', 'said']         NaN  \n",
       "...                                                 ...         ...  \n",
       "1995     ['approaching', 'starlinks', 'active', 'iran']         NaN  \n",
       "1996                               ['couldnt', 'agree']         NaN  \n",
       "1997                                                 []         NaN  \n",
       "1998                                        ['exactly']         NaN  \n",
       "1999  ['many', 'course', 'actively', 'suppressed', '...         NaN  \n",
       "\n",
       "[2000 rows x 7 columns]>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the tweetdata csv file\n",
    "tweet_data = pd.read_csv('tweetdata.csv', encoding = 'latin1')\n",
    "tweet_data.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick up the columnn text and convert it to a list\n",
    "tweet = tweet_data['Tweet'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append all the tweeets space seperated to make a paragraph\n",
    "paragraph = ' '.join(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_corpus = preprocess(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idf_dict is the vocabulary vector create a query vector such that if the word is present in the query, then set the value to 1, where query vector is tweet_cotpus\n",
    "idf_dict = json.load(open('idf.json', 'r'))\n",
    "query_vector = []\n",
    "for token in idf_dict:\n",
    "    if token in tweet_corpus:\n",
    "        query_vector.append(1)\n",
    "    else:\n",
    "        query_vector.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a similarity matrix for the query vector and the tf-idf matrix\n",
    "binary_tf_idf_matrix = pickle.load(open('binary_tf_idf_matrix.pkl', 'rb'))\n",
    "raw_count_tf_idf_matrix = pickle.load(open('raw_count_tf_idf_matrix.pkl', 'rb'))\n",
    "term_frequency_tf_idf_matrix = pickle.load(open('term_frequency_tf_idf_matrix.pkl', 'rb'))\n",
    "log_normalization_tf_idf_matrix = pickle.load(open('log_normalization_tf_idf_matrix.pkl', 'rb'))\n",
    "double_normalization_tf_idf_matrix = pickle.load(open('double_normalization_tf_idf_matrix.pkl', 'rb'))\n",
    "\n",
    "binary_similarity_matrix = cosine_similarity(binary_tf_idf_matrix, [query_vector])\n",
    "raw_count_similarity_matrix = cosine_similarity(raw_count_tf_idf_matrix, [query_vector])\n",
    "term_frequency_similarity_matrix = cosine_similarity(term_frequency_tf_idf_matrix, [query_vector])\n",
    "log_normalization_similarity_matrix = cosine_similarity(log_normalization_tf_idf_matrix, [query_vector])\n",
    "double_normalization_similarity_matrix = cosine_similarity(double_normalization_tf_idf_matrix, [query_vector])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recommend top 10 articles\n",
    "def recommend_top_10_articles(similarity_matrix):\n",
    "    top_10 = np.argsort(similarity_matrix, axis=0)[-10:][::-1]\n",
    "    top_10 = top_10.reshape(10)\n",
    "    return top_10\n",
    "\n",
    "\n",
    "binary_top_10 = recommend_top_10_articles(double_normalization_similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chess/57\n",
      "https://www.slashfilm.com/1229380/the-crown-anchor-pub-from-ted-lasso-is-up-for-rent-on-airbnb/\n",
      "crypto/59\n",
      "https://www.businessinsider.com/elon-musk-history-of-pets-dogs-cats-hobbes-floki-schrodinger-2023-3\n",
      "spacex/34\n",
      "https://www.wired.com/story/the-mystery-vehicle-at-the-heart-of-teslas-new-master-plan/\n",
      "spacex/55\n",
      "https://gizmodo.com/elon-musk-twitter-spacex-morgan-stanley-tesla-1850199556\n",
      "sustainable/31\n",
      "https://www.wired.com/story/the-mystery-vehicle-at-the-heart-of-teslas-new-master-plan/\n",
      "tech/16\n",
      "https://www.engadget.com/the-apple-watch-se-drops-to-its-lowest-price-yet-plus-the-rest-of-the-weeks-best-tech-deals-164528358.html\n",
      "tesla/2\n",
      "https://www.wired.com/story/the-mystery-vehicle-at-the-heart-of-teslas-new-master-plan/\n",
      "tesla/7\n",
      "https://www.businessinsider.com/elon-musk-buy-silicon-valley-bank-tesla-investor-no-thanks-2023-3\n",
      "tesla/72\n",
      "https://www.businessinsider.com/elon-musk-says-ai-dangerous-technology-needs-regulating-2023-3\n",
      "youth activism/59\n",
      "https://www.dazeddigital.com/life-culture/article/58392/1/just-stop-oil-interview-spring-2023-interview-climate-activism\n"
     ]
    }
   ],
   "source": [
    "# use the maplink.json to recommend top 10 news artiles\n",
    "maplink = json.load(open('maplink.json', 'r'))\n",
    "for i, (key, value) in enumerate(maplink.items()):\n",
    "    if i in binary_top_10:\n",
    "        print(key)\n",
    "        print(value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
