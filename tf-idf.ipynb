{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ussin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "import re\n",
    "# import nltk libraries for preprocessing\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "os.chdir('News Articles')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a preprocess function that takes in a string and returns a list of words\n",
    "def preprocess(text):\n",
    "    # remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    # make all words lowercase\n",
    "    text = text.lower()\n",
    "    # split into a list of words\n",
    "    text = text.split()\n",
    "    # remove stopwords\n",
    "    text = [word for word in text if word not in stopwords.words('english')]\n",
    "    # remove words with length==1\n",
    "    text = [word for word in text if len(word)>1]\n",
    "    # lemmatize words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = [lemmatizer.lemmatize(word) for word in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open files in the directory each file contains add en extra section in dictionary of the articles with the key as 'article-id' and vallue as the file name+article number\n",
    "# for example if the file name is 'abc-news.json' and the article number is 1 then the key value pair will be 'article-id':'abc-news/1'\n",
    "# append this key in the dictionary of each article in the file and update the file with the new dictionary as json\n",
    "def update():\n",
    "    for file in os.listdir():\n",
    "        if len(file.split('_')) > 1:\n",
    "                continue\n",
    "        with open(file, 'r') as f:\n",
    "            data_dict = json.load(f)\n",
    "        for i in range(len(data_dict['articles'])):\n",
    "            data_dict['articles'][i]['article-id'] = file.split('.')[0]+'/'+str(i+1)\n",
    "            f.close()\n",
    "        with open(file, 'w') as f:\n",
    "            json.dump(data_dict, f, indent = 4)\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we preprocess the text in each article and store it in a dictionary with key as the article-id and value as the list of words\n",
    "# send the author title description and content of each article to the preprocess function and store the list of words in a dictionary\n",
    "# with key as the article-id and value as the list of words in a new file called 'filename_preprocessed.json'\n",
    "def preprocess_articles():\n",
    "    preprocessed_dict = {}\n",
    "    for file in os.listdir():\n",
    "        print(file)\n",
    "        with open(file, 'r') as f:\n",
    "            if len(file.split('_')) > 1:\n",
    "                continue\n",
    "            data_dict = json.load(f)\n",
    "            for i in range(len(data_dict['articles'])):\n",
    "                # only preprocess article section that are not empty\n",
    "                preprocessed_dict[data_dict['articles'][i]['article-id']] = []\n",
    "                if data_dict['articles'][i]['content'] != None:\n",
    "                    preprocessed_dict[data_dict['articles'][i]['article-id']].extend(preprocess(data_dict['articles'][i]['content']))\n",
    "                if data_dict['articles'][i]['title'] != None:\n",
    "                    preprocessed_dict[data_dict['articles'][i]['article-id']].extend(preprocess(data_dict['articles'][i]['title']))\n",
    "                if data_dict['articles'][i]['description'] != None:\n",
    "                    preprocessed_dict[data_dict['articles'][i]['article-id']].extend(preprocess(data_dict['articles'][i]['description']))\n",
    "                if data_dict['articles'][i]['author'] != None:\n",
    "                    preprocessed_dict[data_dict['articles'][i]['article-id']].extend(preprocess(data_dict['articles'][i]['author']))\n",
    "            f.close()\n",
    "    with open('preprocessed_files.json', 'w') as f:\n",
    "        json.dump(preprocessed_dict, f, indent = 4)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we create a dictionary with key as the document id and value as along with \n",
    "# the link of the article only and no other information and save the whole dict as map.json\n",
    "def create_map():\n",
    "    map_dict = {}\n",
    "    queries = ['crypto', 'tech', 'business', 'spacex', 'tesla', 'politics', 'AI', 'environment', 'youth activism', 'sustainable', 'climate change', 'social justice', 'women rights', 'sports', 'entertainment', 'cricket', 'comedy', 'chess']\n",
    "    for file in os.listdir():\n",
    "        if file.split('.')[0] not in queries:\n",
    "            continue\n",
    "        with open(file, 'r') as f:\n",
    "            data_dict = json.load(f)\n",
    "            for i in range(len(data_dict['articles'])):\n",
    "                map_dict[data_dict['articles'][i]['article-id']] = data_dict['articles'][i]['url']\n",
    "    with open('maplink.json', 'w') as f:\n",
    "        json.dump(map_dict, f, indent = 4)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary for term frequency that has key as the article-id and value as a dictionary with key as the word and value as the frequency of the word in the article\n",
    "def create_tf():\n",
    "    tf_dict = {}\n",
    "    with open('preprocessed_files.json', 'r') as f:\n",
    "        preprocessed_dict = json.load(f)\n",
    "        for key in preprocessed_dict.keys():\n",
    "            tf_dict[key] = {}\n",
    "            for word in preprocessed_dict[key]:\n",
    "                if word in tf_dict[key].keys():\n",
    "                    tf_dict[key][word] += 1\n",
    "                else:\n",
    "                    tf_dict[key][word] = 1\n",
    "    with open('tf.json', 'w') as f:\n",
    "        json.dump(tf_dict, f, indent = 4)\n",
    "        f.close()\n",
    "\n",
    "# create_tf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create posting list that has key as the word and value as a list of article-ids in which the word is present and the number of documents in which the word is present\n",
    "def create_posting_list():\n",
    "    posting_list = {}\n",
    "    with open('preprocessed_files.json', 'r') as f:\n",
    "        preprocessed_dict = json.load(f)\n",
    "        for key in preprocessed_dict.keys():\n",
    "            for word in preprocessed_dict[key]:\n",
    "                if word in posting_list.keys():\n",
    "                    posting_list[word][0].append(key)\n",
    "                    posting_list[word][1] += 1\n",
    "                else:\n",
    "                    posting_list[word] = [[key], 1]\n",
    "    with open('posting_list.json', 'w') as f:\n",
    "        json.dump(posting_list, f, indent = 4)\n",
    "        f.close()\n",
    "\n",
    "# create idf dict\n",
    "def create_idf_dict(no_of_docs, posting_list):\n",
    "    idf_dict = {}\n",
    "    for token in posting_list:\n",
    "        idf_dict[token] = math.log(no_of_docs / posting_list[token][1]+1)\n",
    "    with open('idf.json', 'w') as f:\n",
    "        json.dump(idf_dict, f, indent = 4)\n",
    "        f.close()\n",
    "\n",
    "\n",
    "# create_posting_list()\n",
    "posting_list = json.load(open('posting_list.json', 'r'))\n",
    "number_of_docs = len(json.load(open('preprocessed_files.json', 'r')))\n",
    "# create_idf_dict(number_of_docs, posting_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tf-idf matrix for each document and \n",
    "# Weighting Scheme TF Weight\n",
    "# Binary 0,1\n",
    "# Raw count f(t,d)\n",
    "# Term frequency f(t,d)/Pf(t‘, d)\n",
    "# Log normalization log(1+f(t,d))\n",
    "# Double normalization 0.5+0.5*(f(t,d)/ max(f(t‘,d))\n",
    "\n",
    "def create_tf_idf_matrix_binary(no_of_docs, tf_dict, idf_dict):\n",
    "    vocab_size = len(idf_dict)\n",
    "    tf_idf_matrix = np.zeros((no_of_docs, vocab_size))\n",
    "    # check for all terms in the idf_dict, if it is present in the tf_dict of the document, then set the value to 1*idf\n",
    "    for i, filename in enumerate(tqdm(tf_dict.keys())):\n",
    "        for j, token in enumerate(idf_dict):\n",
    "            if token in tf_dict[filename]:\n",
    "                tf_idf_matrix[i][j] = 1*idf_dict[token]\n",
    "                \n",
    "    return tf_idf_matrix\n",
    "\n",
    "def create_tf_idf_matrix_raw_count(no_of_docs, tf_dict, idf_dict):\n",
    "    vocab_size = len(idf_dict)\n",
    "    tf_idf_matrix = np.zeros((no_of_docs, vocab_size))\n",
    "    for i, filename in enumerate(tqdm(tf_dict.keys())):\n",
    "        for j, token in enumerate(idf_dict):\n",
    "            if token in tf_dict[filename]:\n",
    "                tf_idf_matrix[i][j] = tf_dict[filename][token]*idf_dict[token]\n",
    "                \n",
    "    return tf_idf_matrix\n",
    "\n",
    "def create_tf_idf_matrix_term_frequency(no_of_docs, tf_dict, idf_dict):\n",
    "    vocab_size = len(idf_dict)\n",
    "    tf_idf_matrix = np.zeros((no_of_docs, vocab_size))\n",
    "    for i, filename in enumerate(tqdm(tf_dict.keys())):\n",
    "        for j, token in enumerate(idf_dict):\n",
    "            if token in tf_dict[filename]:\n",
    "                tf_idf_matrix[i][j] = (tf_dict[filename][token]/len(tf_dict[filename]))*idf_dict[token]\n",
    "\n",
    "    return tf_idf_matrix\n",
    "\n",
    "def create_tf_idf_matrix_log_normalization(no_of_docs, tf_dict, idf_dict):\n",
    "    vocab_size = len(idf_dict)\n",
    "    tf_idf_matrix = np.zeros((no_of_docs, vocab_size))\n",
    "    for i, filename in enumerate(tqdm(tf_dict.keys())):\n",
    "        for j, token in enumerate(idf_dict):\n",
    "            if token in tf_dict[filename]:\n",
    "                tf_idf_matrix[i][j] = math.log(1+tf_dict[filename][token])*idf_dict[token]\n",
    "    \n",
    "    return tf_idf_matrix\n",
    "\n",
    "def create_tf_idf_matrix_double_normalization(no_of_docs, tf_dict, idf_dict):\n",
    "    vocab_size = len(idf_dict)\n",
    "    tf_idf_matrix = np.zeros((no_of_docs, vocab_size))\n",
    "    for i, filename in enumerate(tqdm(tf_dict.keys())):\n",
    "        max_tf = 0\n",
    "        for token in tf_dict[filename]:\n",
    "            if tf_dict[filename][token] > max_tf:\n",
    "                max_tf = tf_dict[filename][token]\n",
    "        for j, token in enumerate(idf_dict):\n",
    "            if token in tf_dict[filename]:\n",
    "                tf_idf_matrix[i][j] = 0.5+0.5*(tf_dict[filename][token]/max_tf)*idf_dict[token]\n",
    "    return tf_idf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1800/1800 [00:04<00:00, 411.36it/s]\n",
      "100%|██████████| 1800/1800 [00:03<00:00, 456.78it/s]\n",
      "100%|██████████| 1800/1800 [00:04<00:00, 419.98it/s]\n",
      "100%|██████████| 1800/1800 [00:04<00:00, 429.00it/s]\n",
      "100%|██████████| 1800/1800 [00:04<00:00, 401.51it/s]\n"
     ]
    }
   ],
   "source": [
    "# create tf-idf matrix\n",
    "binary_tf_idf_matrix = create_tf_idf_matrix_binary(number_of_docs, json.load(open('tf.json', 'r')), json.load(open('idf.json', 'r')))\n",
    "raw_count_tf_idf_matrix = create_tf_idf_matrix_raw_count(number_of_docs, json.load(open('tf.json', 'r')), json.load(open('idf.json', 'r')))\n",
    "term_frequency_tf_idf_matrix = create_tf_idf_matrix_term_frequency(number_of_docs, json.load(open('tf.json', 'r')), json.load(open('idf.json', 'r')))\n",
    "log_normalization_tf_idf_matrix = create_tf_idf_matrix_log_normalization(number_of_docs, json.load(open('tf.json', 'r')), json.load(open('idf.json', 'r')))\n",
    "double_normalization_tf_idf_matrix = create_tf_idf_matrix_double_normalization(number_of_docs, json.load(open('tf.json', 'r')), json.load(open('idf.json', 'r')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the similarity matrix in a pickle file\n",
    "# with open('binary_tf_idf_matrix.pkl', 'wb') as f:\n",
    "#     pickle.dump(binary_tf_idf_matrix, f)\n",
    "# with open('raw_count_tf_idf_matrix.pkl', 'wb') as f:\n",
    "#     pickle.dump(raw_count_tf_idf_matrix, f)\n",
    "# with open('term_frequency_tf_idf_matrix.pkl', 'wb') as f:\n",
    "#     pickle.dump(term_frequency_tf_idf_matrix, f)\n",
    "# with open('log_normalization_tf_idf_matrix.pkl', 'wb') as f:\n",
    "#     pickle.dump(log_normalization_tf_idf_matrix, f)\n",
    "# with open('double_normalization_tf_idf_matrix.pkl', 'wb') as f:\n",
    "#     pickle.dump(double_normalization_tf_idf_matrix, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800\n"
     ]
    }
   ],
   "source": [
    "# load the tweetdata csv file\n",
    "tweet_data = pd.read_csv('LemftData.csv', encoding = 'latin1')\n",
    "tweet_data.head\n",
    "print(number_of_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick up the columnn text and convert it to a list\n",
    "tweet = tweet_data['Tweet'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append all the tweeets space seperated to make a paragraph\n",
    "paragraph = ' '.join(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_corpus = preprocess(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idf_dict is the vocabulary vector create a query vector such that if the word is present in the query, then set the value to 1, where query vector is tweet_cotpus\n",
    "idf_dict = json.load(open('idf.json', 'r'))\n",
    "query_vector = []\n",
    "for token in idf_dict:\n",
    "    if token in tweet_corpus:\n",
    "        query_vector.append(1)\n",
    "    else:\n",
    "        query_vector.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a similarity matrix for the query vector and the tf-idf matrix\n",
    "# binary_tf_idf_matrix = pickle.load(open('binary_tf_idf_matrix.pkl', 'rb'))\n",
    "# raw_count_tf_idf_matrix = pickle.load(open('raw_count_tf_idf_matrix.pkl', 'rb'))\n",
    "# term_frequency_tf_idf_matrix = pickle.load(open('term_frequency_tf_idf_matrix.pkl', 'rb'))\n",
    "# log_normalization_tf_idf_matrix = pickle.load(open('log_normalization_tf_idf_matrix.pkl', 'rb'))\n",
    "# double_normalization_tf_idf_matrix = pickle.load(open('double_normalization_tf_idf_matrix.pkl', 'rb'))\n",
    "\n",
    "binary_similarity_matrix = cosine_similarity(binary_tf_idf_matrix, [query_vector])\n",
    "raw_count_similarity_matrix = cosine_similarity(raw_count_tf_idf_matrix, [query_vector])\n",
    "term_frequency_similarity_matrix = cosine_similarity(term_frequency_tf_idf_matrix, [query_vector])\n",
    "log_normalization_similarity_matrix = cosine_similarity(log_normalization_tf_idf_matrix, [query_vector])\n",
    "double_normalization_similarity_matrix = cosine_similarity(double_normalization_tf_idf_matrix, [query_vector])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recommend top 10 articles\n",
    "def recommend_top_10_articles(similarity_matrix):\n",
    "    top_10 = np.argsort(similarity_matrix, axis=0)[-10:][::-1]\n",
    "    top_10 = top_10.reshape(10)\n",
    "    return top_10\n",
    "\n",
    "binary_top_10 = recommend_top_10_articles(double_normalization_similarity_matrix)\n",
    "\n",
    "# recommend top 50 articles\n",
    "# def recommend_top_50_articles(similarity_matrix):\n",
    "#     top_50 = np.argsort(similarity_matrix, axis=0)[-50:][::-1]\n",
    "#     top_50 = top_50.reshape(50)\n",
    "#     return top_50\n",
    "\n",
    "# binary_top_10 = recommend_top_50_articles(double_normalization_similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chess/39\n",
      "https://www.cnet.com/health/mental/these-8-hobbies-will-help-your-mental-health-spring-forward/\n",
      "chess/46\n",
      "https://indianexpress.com/article/trending/trending-in-india/anand-mahindra-boy-who-travelled-all-night-chess-tournament-8471534/\n",
      "chess/57\n",
      "https://www.slashfilm.com/1229380/the-crown-anchor-pub-from-ted-lasso-is-up-for-rent-on-airbnb/\n",
      "entertainment/21\n",
      "https://www.businessinsider.com/personal-finance/capital-one-entertainment-tickets-march-madness-mlb-2023-3\n",
      "social justice/12\n",
      "https://www.theguardian.com/social-work-looking-to-the-future/2023/mar/13/making-a-difference-social-workers-share-their-career-highlights\n",
      "social justice/17\n",
      "https://www.theguardian.com/global-development/2023/mar/08/happy-international-womens-day-a-look-back-at-over-a-century-of-the-global-fight-for-justice-and-equality\n",
      "tech/70\n",
      "https://readwrite.com/tech-hiring/\n",
      "women rights/4\n",
      "https://www.theguardian.com/global-development/2023/mar/08/happy-international-womens-day-a-look-back-at-over-a-century-of-the-global-fight-for-justice-and-equality\n",
      "women rights/15\n",
      "https://www.businessinsider.com/hershey-international-womens-day-campaign-trans-woman-chocolate-candy-bars-2023-3\n",
      "youth activism/67\n",
      "https://nypost.com/2023/03/04/woke-ideologies-are-upending-american-childhood/\n"
     ]
    }
   ],
   "source": [
    "# use the maplink.json to recommend top 10 news artiles\n",
    "maplink = json.load(open('maplink.json', 'r'))\n",
    "for i, (key, value) in enumerate(maplink.items()):\n",
    "    if i in binary_top_10:\n",
    "        print(key)\n",
    "        print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.04071625]\n",
      " [0.08862927]\n",
      " [0.03104914]\n",
      " ...\n",
      " [0.04493081]\n",
      " [0.01117823]\n",
      " [0.02981701]]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
